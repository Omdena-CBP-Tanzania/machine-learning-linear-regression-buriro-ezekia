# -*- coding: utf-8 -*-
"""Predicting House Prices Using Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1embaaVn29DOWSm3CQrMbyEevRmO-PpAG

**Project Title**: Predicting House Prices Using Linear Regression

**Objective**: To introduce students to supervised learning, focusing on linear regression, by guiding them through a project that predicts house prices based on a variety of features.

# **Data Exploration and Preprocessing**

**Task 2: Data Preprocessing**
"""

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
# Assuming the dataset is stored in a CSV file named 'BostonHousing.csv' in the specified path
df = pd.read_csv('drive/MyDrive/Datasets/BostonHousing.csv')

# Identify missing values
missing_values = df.isnull().sum()
print("Missing Values:")
print(missing_values)

# Impute missing values
# For simplicity, we'll use mean imputation for numerical features
df.fillna(df.mean(), inplace=True)

# Verify that there are no missing values left
print("\nMissing Values after Imputation:")
print(df.isnull().sum())

# Detect outliers using box plots for each feature
features = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat', 'medv']

# # Create subplots for each feature to identify outliers
# fig, axes = plt.subplots(nrows=7, ncols=2, figsize=(15, 30))
# axes = axes.flatten()

# for i, feature in enumerate(features):
#     sns.boxplot(x=df[feature], ax=axes[i])
#     axes[i].set_title(f'Boxplot of {feature}')
#     axes[i].set_xlabel(feature)

# # Adjust layout
# plt.tight_layout()
# plt.show()

# Handle outliers by capping them at the 1st and 99th percentiles
for feature in features:
    lower_percentile = df[feature].quantile(0.01)
    upper_percentile = df[feature].quantile(0.99)
    df[feature] = df[feature].clip(lower=lower_percentile, upper=upper_percentile)

# Verify outliers are handled
print("\nSummary Statistics after handling outliers:")
print(df.describe())

# Encode categorical variables
# For this dataset, the only categorical variable is 'chas'
df = pd.get_dummies(df, columns=['chas'], drop_first=True)

# Verify encoding
print("\nData after encoding categorical variables:")
print(df.head())

# Normalize/standardize numerical features.
# Import necessary libraries
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Separate features and target variable
X = df.drop(columns=['medv'])
y = df['medv']

# Normalize numerical features
scaler = MinMaxScaler()
X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

print("\nNormalized Features:")
print(X_normalized.head())

# Standardize numerical features
scaler = StandardScaler()
X_standardized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

print("\nStandardized Features:")
print(X_standardized.head())

# Split the data into training and testing sets
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split

# # Separate features and target variable
# X = df.drop(columns=['medv'])
# y = df['medv']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Save the training and testing sets to the drive
train_data = pd.concat([X_train, y_train], axis=1)
test_data = pd.concat([X_test, y_test], axis=1)

train_data.to_csv('drive/MyDrive/Datasets/BostonHousing_train.csv', index=False)
test_data.to_csv('drive/MyDrive/Datasets/BostonHousing_test.csv', index=False)

print("Training and testing data saved successfully.")

# Load the saved trained and test dataset (Optional)
train_data = pd.read_csv('drive/MyDrive/Datasets/BostonHousing_train.csv')
test_data = pd.read_csv('drive/MyDrive/Datasets/BostonHousing_test.csv')

# Check the size of the trained and tested dataset
print("Size of the training dataset:", train_data.shape)
print("Size of the testing dataset:", test_data.shape)